import pandas as pd
from pathlib import Path

# Import the necessary functions from our new packages
from sample_importer.readers import load_all_sheets_from_files
from sample_importer.harmonizers import create_harmonized_sample_list

# --- Configuration ---
# Define the location of the sensitive linkage key file.
# This file is generated by the `deidentification_tool`.
LINKAGE_KEY_PATH = Path(__file__).parent / 'SECURE_linkage_key.csv'

# Define the output path for the final, de-identified data.
OUTPUT_TSV_PATH = Path(__file__).parent / 'harmonized_deidentified_samples.tsv'

def create_lookup_from_linkage_key(key_path):
    """
    Loads the sensitive linkage key and creates a lookup map from any legacy ID
    to the new, random subject_id.
    """
    if not key_path.exists():
        print(f"Error: Linkage key file not found at {key_path}")
        print("Please run the deidentification_tool first to generate it.")
        return None

    print(f"Loading linkage key from {key_path}...")
    # Explicitly read identifier columns as strings to avoid type mismatches (e.g., '123' vs '123.0')
    linkage_df = pd.read_csv(key_path, dtype={'mrn': str, 'upn': str, 'pmid': str})
    
    legacy_to_new_id_map = {}
    for _, row in linkage_df.iterrows():
        new_id = row['subject_id']
        for id_type in ['mrn', 'upn', 'pmid']:
            # Handle cases where a person might not have all ID types
            if pd.notna(row[id_type]):
                for legacy_id in str(row[id_type]).split(';'):
                    if legacy_id:
                        legacy_to_new_id_map[legacy_id] = new_id
    print("ID lookup map created.")
    return legacy_to_new_id_map

def main():
    """
    Main function to execute the harmonization process.
    """
    # --- 1. Create the ID lookup map ---
    id_lookup = create_lookup_from_linkage_key(LINKAGE_KEY_PATH)
    if id_lookup is None:
        return # Stop if the linkage key wasn't found

    # --- 2. Load the raw data ---
    # As before, we use the simulated data for this example.
    # In a real scenario, you would provide the paths to your actual data files.
    # file_paths = ["path/to/data/project_A.xlsx", "path/to/data/project_B.xlsx"]
    # all_dfs_raw = load_all_sheets_from_files(file_paths)
    data_a = {'MRN': ['11223344', '11223344', '55667788'], 'Internal_ID': ['PMID-01', 'PMID-01', 'PMID-02'], 'Collection Date': ['10/05/2024', '11/15/2024', '10/08/2024'], 'Sample Type': ['Stool', 'Skin', 'stool'], 'FASTQ': ['p01_s1.fq.gz', 'p01_s2.fq.gz', 'p02_s1.fq.gz']}
    df_a = pd.DataFrame(data_a)
    data_b = {'UPN': ['UPN_A5', 'UPN_B9', 'UPN_B9'], 'PMID': ['PMID-03', 'PMID-04', 'PMID-04'], 'collection_dt': pd.to_datetime(['2023-11-01', '2023-11-02', '2023-12-01']), 'source': ['oral', 'stool', 'stool'], 'sequence_file': ['p03_s1.fq.gz', 'p04_s1.fq.gz', 'p04_s2.fq.gz'], 'Project': ['IBD_Cohort', 'IBD_Cohort', 'IBD_Cohort']}
    df_b = pd.DataFrame(data_b)
    data_c = {'patient_id': ['PMID-02', 'PMID-04', 'PMID-05'], 'sample_date': ['2024-01-20', '2024-02-15', '2024-03-10'], 'sample_type': ['stool', 'stool', 'skin'], 'filename': ['old_p02.fq.gz', 'old_p04.fq.gz', 'old_p05.fq.gz'], 'PI': ['Dr. Jones', 'Dr. Smith', 'Dr. Jones']}
    df_c = pd.DataFrame(data_c)
    all_dfs_raw = {
        'clinical_research_A.xlsx|Sheet1': df_a,
        'collab_lab_B.xlsx|2023_Samples': df_b,
        'collab_lab_B.xlsx|2024_Samples': df_c,
    }

    # --- 3. Execute Phase 2 (Harmonization) ---
    harmonized_samples = create_harmonized_sample_list(all_dfs_raw, id_lookup)
    
    # --- 4. Save the final de-identified data ---
    if harmonized_samples is not None:
        harmonized_samples.to_csv(OUTPUT_TSV_PATH, sep='\t', index=False)
        print(f"\n--- Harmonized Samples Output (`{OUTPUT_TSV_PATH}`) ---")
        print("This file is de-identified and ready for analysis and database import.")
        print(harmonized_samples)

if __name__ == "__main__":
    main()
